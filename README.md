
## 前言
在做渗透测试时，一般甲方都会直接扔上万个url，然后需要我们从中挖到漏洞。那么第一步就是从这些url中筛选出正常的网站，并且简单的筛选出哪些网站是可值得去挖掘的，哪些网站是可能存在的，但是页面显示404，403等，哪些网站是不存在的直接就可以放弃的。

## 项目介绍
我这里用了1.txt作为测试目标，大概有一万多个ip。

![image](https://s2.ax1x.com/2020/02/19/3VrbwD.png)

### 构思

本人最初做这个项目的时候，代码能力很薄弱（虽然现在也好不到哪去），只能思考到简单的for循环一个一个去尝试，request到如果是200，3xx就表示一个正常的网站，其他都是不存在的。但是后来师傅替我引导了一下，举了一个简单的例子。

咋们去访问https://www.baidu.com，返回的状态码是200，那如果访问https://www.baidu.com/dfsfsfdfs呢？（域名后面的目录是随机的）我起初想的是应该是404了吧。可是实际上返回的并不是404。

![image](https://s2.ax1x.com/2020/02/19/3VDdv6.png)

并且可以看到这里的url也并不是我们之前访问的url，而是进入了百度预先设定好的error.html，状态码变成了304。也就是说我们访问了一个不存在的页面，百度收到我们的请求，但是并不是直接返回404，而是先是经过解析判断了是否存在该目录或文件，若不存在就呈现这个error的页面给我们。但是实际上我们需要知道的是百度后端是已经处理了我们的请求了,然后去进行了判断返回了304和error页面给我们，难道我们就能片面的说百度因为返回了一个错误的页面或者非200的页面就说该网站不存在吗？这显然是不合情理的。

所以我在大佬的引导下写了一个如何判断网站是否存在的简单的思路图。

![image](https://s2.ax1x.com/2020/02/19/3V29IO.png)

当然在实际情况下并没有这么简单，在最后的结果还有很多细节需要校验，修正。

然后思路大致方向有了后，就该想想代码效率问题了。对网站的请求和回应会经过http协议，这样就会出现一个等待时间。如果一个一个的顺序去请求，那得跑到何年何月。于是我就想着要么换成多线程或者异步协程。这两者的区别和优缺点在这里就不说了。当时我开始的时候用两个方法都写了一遍。发现异步协程在这种情况下确实比多线程快的多。

### 代码讲解

![image](https://s2.ax1x.com/2020/02/19/3V4K2D.png)
在python3.8中是可以直接asyncio.run()来启动的，但是在低版本中就得创建loop。

![image](https://s2.ax1x.com/2020/02/19/3V40MQ.png)

verdictUrl()函数是将1.txt中的url文件拿出来放进asyncio.Queue()的队列里面。然后用aiohttp.ClientSession作为requests中的seesion来访问url。然后开启100个并发，将session和队列都放进get_title中。await asyncio.wait(tasks)开启异步协程。（这里建议并发的数量在50到100中间，因为小于50会增加脚本时间，大于100可能会对结果有影响）至于为什么用aiohttp不用httpx是本人发现httpx对https协议的网站不是很友好，请求的时候不是很准确，会报ssl的错，也是因为本人太菜了，没有找到好的解决办法。

![image](https://s2.ax1x.com/2020/02/19/3Vo1r4.png)

在get_title函数里面我对常出现的状态进行了分类处理，可以看到特别是随机目录访问的时候的状态码是404和200的时候会单独进入判断就是为了判断网站的准确性。

至于这里为什么会有个driver，我解释一下。

![image](https://s2.ax1x.com/2020/02/19/3VTyXF.png)

因为python的requests访问和浏览器直接访问是有一定区别的，况且在异步加载中可能会出现数据丢失的情况，也就是返回的代码和浏览器中的有差别。所以在某些地方我会用headless chrome去访问，获取title。

![image](https://s2.ax1x.com/2020/02/19/3VTXtI.png)
![image](https://s2.ax1x.com/2020/02/19/3V75Us.png)

在four0four和two00函数中也会对根目录请求的状态码进行讨论确保结论的准确高效。

特别是在随机访问200，根目录访问200的情况下，我用了content_length和返回的源码长度去判断该网站是否是正常的，因为有些网站访问全是200的有可能是有自己设置的404页面，后端是有进行处理的，也有可能是这个网站什么都没有访问什么都是空。这些细节特别需要注意，因为会为后期的渗透省下一些时间。

最后说一下对url和结果的保存使用了字典的形式，url是键，title是值。

## 结果

最后将处理完的结果保存成四个文件A,B,C,D。A是存在的正常网站，B是可能存在的网站，C是极大可能不存在的网站，D不是一个网站

![image](https://s2.ax1x.com/2020/02/19/3VLhQ0.png)

![image](https://s2.ax1x.com/2020/02/19/3VLXS1.png)
![image](https://s2.ax1x.com/2020/02/19/3VOFfA.png)

![image](https://s2.ax1x.com/2020/02/19/3VOJXV.png)

花费时间大概在16分钟到21分钟之间。

## 有待改进之处

1.脚本获取title时有些会获取不完整，会导致标题为空。

2.异步还是容易会出现漏包的情况。以后会慢慢完善代码。

## 结束语

通过这次的工具编写，让我意识到了自己代码能力还是太弱，需要长时间锻炼。其实就是对网站一些细节的地方进行了深入的了解。

最后：如果大佬们有更好的思路或者见解，欢迎来交流！